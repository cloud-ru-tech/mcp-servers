import re
from urllib.parse import urlparse
import httpx
from bs4 import BeautifulSoup

import uvicorn
from starlette.applications import Starlette
from starlette.requests import Request
from starlette.routing import Route, Mount

from mcp.server.fastmcp import FastMCP
from mcp.shared.exceptions import McpError
from mcp.types import ErrorData, INTERNAL_ERROR, INVALID_PARAMS
from mcp.server.sse import SseServerTransport

# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä MCP —Å–µ—Ä–≤–µ—Ä–∞ —Å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–º "fetch"
mcp = FastMCP("fetch")


def clean_text(text: str) -> str:
    """
    –û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫
    
    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        
    Returns:
        –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    # –°–Ω–∞—á–∞–ª–∞ —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
    text = re.sub(r'\n\s*\n+', '\n', text.strip())
    # –ó–∞—Ç–µ–º —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –≤ —Å—Ç—Ä–æ–∫–∞—Ö, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã
    lines = text.split('\n')
    cleaned_lines = [re.sub(r'\s+', ' ', line.strip()) for line in lines if line.strip()]
    return '\n'.join(cleaned_lines)


def extract_text_content(html: str, base_url: str = "") -> str:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∏–∑ HTML, —É–±–∏—Ä–∞—è –≤—Å–µ —Ç–µ–≥–∏
    
    Args:
        html: HTML –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        base_url: –ë–∞–∑–æ–≤—ã–π URL –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
        
    Returns:
        –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ HTML —Ç–µ–≥–æ–≤
    """
    try:
        soup = BeautifulSoup(html, 'html.parser')
        
        # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã, —Å—Ç–∏–ª–∏ –∏ –¥—Ä—É–≥–∏–µ –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        unwanted_elements = [
            'script', 'style', 'meta', 'link', 'noscript', 
            'header', 'footer', 'nav'
        ]
        for element in soup(unwanted_elements):
            element.decompose()
        
        # –ü–æ–ª—É—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        title = soup.find('title')
        title_text = title.get_text() if title else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
        # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ç–µ–Ω—Ç–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏
        main_content = (
            soup.find('main') or 
            soup.find('article') or 
            soup.find('div', class_=re.compile(r'content|main', re.I))
        )
        
        if main_content:
            content_text = main_content.get_text(separator=' ', strip=True)
        else:
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –æ—Å–Ω–æ–≤–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏, –±–µ—Ä–µ–º –≤–µ—Å—å body
            body = soup.find('body')
            if body:
                content_text = body.get_text(separator=' ', strip=True)
            else:
                content_text = soup.get_text(separator=' ', strip=True)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Ç–µ–∫—Å—Ç
        result = f"–ó–ê–ì–û–õ–û–í–û–ö: {title_text}\n\n–°–û–î–ï–†–ñ–ò–ú–û–ï:\n{content_text}"
        
        return clean_text(result)
        
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞: {str(e)}"


async def fetch_page_content(url: str, timeout: int = 30) -> str:
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç
    
    Args:
        url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è
        timeout: –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        
    Returns:
        –¢–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    """
    try:
        # –í–∞–ª–∏–¥–∞—Ü–∏—è URL
        parsed_url = urlparse(url)
        if not parsed_url.scheme or not parsed_url.netloc:
            raise McpError(
                ErrorData(
                    code=INVALID_PARAMS,
                    message=f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL: {url}"
                )
            )
        
        # –î–æ–±–∞–≤–ª—è–µ–º https:// –µ—Å–ª–∏ —Å—Ö–µ–º–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞
        if not url.startswith(('http://', 'https://')):
            url = f"https://{url}"
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –±—Ä–∞—É–∑–µ—Ä–∞
        headers = {
            'User-Agent': (
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                'AppleWebKit/537.36 (KHTML, like Gecko) '
                'Chrome/91.0.4472.124 Safari/537.36'
            ),
            'Accept': (
                'text/html,application/xhtml+xml,application/xml;q=0.9,'
                'image/webp,*/*;q=0.8'
            ),
            'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        async with httpx.AsyncClient(
            timeout=timeout,
            headers=headers,
            follow_redirects=True
        ) as client:
            response = await client.get(url)
            response.raise_for_status()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏ HTML –∫–æ–Ω—Ç–µ–Ω—Ç
            content_type = response.headers.get('content-type', '').lower()
            if ('text/html' not in content_type and 
                    'application/xhtml' not in content_type):
                content_preview = response.text[:2000]
                if len(response.text) > 2000:
                    content_preview += '...'
                return (
                    f"–í–Ω–∏–º–∞–Ω–∏–µ: –ü–æ–ª—É—á–µ–Ω –Ω–µ HTML –∫–æ–Ω—Ç–µ–Ω—Ç "
                    f"(content-type: {content_type})\n\n"
                    f"–°–æ–¥–µ—Ä–∂–∏–º–æ–µ:\n{content_preview}"
                )
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
            text_content = extract_text_content(response.text, url)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
            page_info = (
                f"URL: {url}\n"
                f"–°—Ç–∞—Ç—É—Å: {response.status_code}\n"
                f"–†–∞–∑–º–µ—Ä: {len(response.text)} —Å–∏–º–≤–æ–ª–æ–≤\n"
                f"–ö–æ–¥–∏—Ä–æ–≤–∫–∞: {response.encoding or 'auto'}\n"
                f"–ü–æ—Å–ª–µ–¥–Ω–µ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ: "
                f"{response.headers.get('last-modified', '–Ω–µ —É–∫–∞–∑–∞–Ω–æ')}\n"
            )
            page_info += "=" * 50 + "\n\n"
            
            return page_info + text_content
            
    except httpx.TimeoutException:
        raise McpError(
            ErrorData(
                code=INTERNAL_ERROR,
                message=f"–ü—Ä–µ–≤—ã—à–µ–Ω —Ç–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è URL: {url}"
            )
        )
    except httpx.HTTPStatusError as e:
        raise McpError(
            ErrorData(
                code=INTERNAL_ERROR,
                message=f"HTTP –æ—à–∏–±–∫–∞ {e.response.status_code} –¥–ª—è URL: {url}"
            )
        )
    except Exception as e:
        raise McpError(
            ErrorData(
                code=INTERNAL_ERROR,
                message=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {str(e)}"
            )
        )


@mcp.tool()
async def fetch_page(url: str, timeout: int = 30) -> str:
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç –±–µ–∑ HTML/CSS/JS
    
    Args:
        url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è (—Å –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–º http:// –∏–ª–∏ https://)
        timeout: –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 30)
    
    Returns:
        –¢–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –±–µ–∑ HTML —Ä–∞–∑–º–µ—Ç–∫–∏
    """
    
    if not url:
        return "–û—à–∏–±–∫–∞: URL –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º"
    
    if timeout <= 0 or timeout > 120:
        return "–û—à–∏–±–∫–∞: –¢–∞–π–º–∞—É—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç 1 –¥–æ 120 —Å–µ–∫—É–Ω–¥"
    
    try:
        content = await fetch_page_content(url, timeout)
        return content
    except McpError:
        raise
    except Exception as e:
        return f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {str(e)}"


# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ SSE —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞
sse = SseServerTransport("/messages/")


async def handle_sse(request: Request):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ SSE —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π"""
    _server = mcp._mcp_server
    async with sse.connect_sse(
        request.scope,
        request.receive,
        request._send,
    ) as (reader, writer):
        await _server.run(
            reader, 
            writer, 
            _server.create_initialization_options()
        )


# –°–æ–∑–¥–∞–µ–º Starlette –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
app = Starlette(
    debug=True,
    routes=[
        Route("/sse", endpoint=handle_sse),
        Mount("/messages/", app=sse.handle_post_message),
    ],
)

if __name__ == "__main__":
    print("üåê –ó–∞–ø—É—Å–∫ MCP Fetch —Å–µ—Ä–≤–µ—Ä–∞...")
    print("üì° SSE endpoint: http://localhost:8002/sse")
    print("üîß Tools: fetch_page")
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8002,
        log_level="info"
    ) 